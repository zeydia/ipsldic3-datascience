{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc31b0d4-2bda-4e59-b351-af7134a17ffe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Data  Engineering project on tripdatas 2024 IPSLGIT32026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce566fd5-deff-41ae-bc86-c4f6660e6984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3a53ba-d136-4250-938d-8be16ae6a9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/Volumes/workspace/ipsldic3/mainvol/yellostaxidataset20024/yellow_tripdata_2024-01.parquet\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67804da0-c533-458d-9153-43122571c1ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acdfd8de-2642-4731-b9c5-94601b34df29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Directory config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccaec575-6d23-4503-b38f-3d2485e3e9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"ipsldic3\"\n",
    "VOLUME = \"capstoneipsl\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "736310cf-5243-4678-a8a1-7ab2f2ff1569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Volume if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47352343-9997-49b4-ae30-e216ef38d9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{VOLUME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3931a1ca-c96e-43da-b564-0bee70748467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Directories for Medallion Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9584a526-1574-4fac-a1a1-a8f281a83ea0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Define Paths\n",
    "VOLUME_ROOT = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}\"\n",
    "BRONZE = f\"{VOLUME_ROOT}/yellow_taxi/bronze\"\n",
    "SILVER = f\"{VOLUME_ROOT}/yellow_taxi/silver\"\n",
    "GOLD   = f\"{VOLUME_ROOT}/yellow_taxi/gold\"\n",
    "CHECKPOINTS = f\"{VOLUME_ROOT}/yellow_taxi/_checkpoints\"\n",
    "\n",
    "# 3. Reset/Create Directories (Using standard Python os/shutil or dbutils)\n",
    "# Note: dbutils.fs.mkdirs works on Volumes too\n",
    "dbutils.fs.mkdirs(BRONZE)\n",
    "dbutils.fs.mkdirs(SILVER)\n",
    "dbutils.fs.mkdirs(GOLD)\n",
    "dbutils.fs.mkdirs(CHECKPOINTS)\n",
    "\n",
    "print(f\"Created Volume directories at: {VOLUME_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d37f0f60-b704-4783-846c-bc5b9f5f3b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab819e98-9354-4a21-a8ea-89276373eb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ingestion initiale des donnees brutes\n",
    "BRONZE depuis un fichier externe ou en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d9b705-9109-4641-a28d-bcfbc72ffbbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Download Sample Data (January 2024 Yellow Taxi)\n",
    "# Use dbutils.fs.cp to copy from external location if needed, otherwise use %sh for wget\n",
    "\n",
    "# Define the correct path using the VOLUME and BRONZE variables\n",
    "RAW_PATH = f\"{BRONZE}/raw\"\n",
    "dbutils.fs.mkdirs(RAW_PATH)\n",
    "\n",
    "# Download the file using wget in a shell command\n",
    "# Note: %sh cannot use Python variables directly, so we use f-string to construct the shell command\n",
    "\n",
    "# file_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\"\n",
    "file_url = \"/Volumes/workspace/ipsldic3/mainvol/yellostaxidataset20024/yellow_tripdata_2024-02.parquet\"\n",
    "local_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/yellow_taxi/bronze/raw/yellow_2024_02.parquet\"\n",
    "\n",
    "# Use Python to run shell commands for dynamic paths\n",
    "import subprocess\n",
    "\n",
    "# subprocess.run(f\"wget -q -O {local_path} {file_url}\", shell=True, check=True)\n",
    "subprocess.run(f\"cp {file_url} {local_path}\", shell=True, check=True)\n",
    "subprocess.run(f\"ls -lh {RAW_PATH}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dc27c43-1915-424a-b232-ea3c783e598d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"path\":682},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1769650721771}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(RAW_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312a3461-5bb5-4c4b-8fd8-44503a17baa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Load datas in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2592d23-cf6b-498d-8052-fa580f9826cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# 5. Load the raw dataframe\n",
    "# This variable `df_raw` will be the starting point.\n",
    "df_raw = spark.read.parquet(f\"{BRONZE}/raw/yellow_2024_02.parquet\")\n",
    "print(\"Bootstrap complete. df_raw loaded from Volume.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b515a21e-41d8-47a0-9df6-26ace7564d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_raw.limit(5))\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ae167e8-82a2-4203-aeb6-5d888942af16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyse statistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4949add-6178-47cc-9537-83f7ed8c4319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Load raw data (reuse df_raw from bootstrap or load again)\n",
    "df = df_raw\n",
    "\n",
    "# Step 2: Basic profiling\n",
    "print(\"rows:\", df.count())\n",
    "print(\"columns:\", len(df.columns))\n",
    "df.describe([\"trip_distance\", \"fare_amount\", \"total_amount\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d55b90-f984-49bd-9f6d-f2cdcfc3aaf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Audit de qualite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4e62ff6-920b-41b4-b585-ca5b1c2d1139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Make a table with: column_name, spark_type, null_count, example_value.\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "rows = []\n",
    "total_count = df.count()\n",
    "for field in df.schema.fields:\n",
    "    colname = field.name\n",
    "    nulls = df.filter(F.col(colname).isNull()).count()\n",
    "    example = df.select(colname).where(F.col(colname).isNotNull()).limit(1).collect()\n",
    "    example_val = example[0][0] if example else None\n",
    "    rows.append((colname, field.dataType.simpleString(), nulls, str(example_val),total_count))\n",
    "\n",
    "dict_df = spark.createDataFrame(rows, [\"column\", \"type\", \"null_count\", \"example\",\"total_count\"])\n",
    "display(dict_df.orderBy(\"column\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f45963-dc4b-487b-a482-d9044cc0694c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyse des plans d'execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f887cf-1dec-40ac-8d26-10c6b70761c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 13"
    }
   },
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "# A shuffle-heavy job: groupBy\n",
    "q = (df.groupBy(\"payment_type\")\n",
    "       .agg(F.count(\"*\").alias(\"trips\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_total\")))\n",
    "\n",
    "q.explain(True)  # view logical + physical plan\n",
    "q.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf2e108-fdd7-455b-b755-6369d00a7df5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analyse des temps de trajets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27487324-7fbf-4336-8945-882d9e1b25c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 14"
    }
   },
   "outputs": [],
   "source": [
    "# cleansing\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = df_raw\n",
    "\n",
    "df_feat = (df\n",
    "  .withColumn(\"trip_date\", F.to_date(\"tpep_pickup_datetime\"))\n",
    "  .withColumn(\"trip_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "  .withColumn(\"trip_duration_min\",\n",
    "      (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60.0\n",
    "  )\n",
    "  .withColumn(\"is_cash\", (F.col(\"payment_type\") == 2).cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Quick QC\n",
    "df_feat.select(\"trip_date\",\"trip_hour\",\"trip_duration_min\",\"trip_distance\",\"total_amount\").describe().show()\n",
    "display(df_feat.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd5be5bf-b899-44a5-9014-d7ec0ff2c556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Analtics\n",
    "df_feat.createOrReplaceTempView(\"yellow\")\n",
    "\n",
    "# Daily KPIs\n",
    "daily = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  trip_date,\n",
    "  COUNT(*) AS trips,\n",
    "  AVG(trip_distance) AS avg_distance,\n",
    "  AVG(total_amount) AS avg_total\n",
    "FROM yellow\n",
    "GROUP BY trip_date\n",
    "ORDER BY trip_date\n",
    "\"\"\")\n",
    "display(daily)\n",
    "\n",
    "# Hourly KPIs\n",
    "hourly = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  trip_hour,\n",
    "  COUNT(*) AS trips,\n",
    "  AVG(total_amount) AS avg_total\n",
    "FROM yellow\n",
    "GROUP BY trip_hour\n",
    "ORDER BY trip_hour\n",
    "\"\"\")\n",
    "display(hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2b5259-0340-46bd-8e9b-5fa347d28585",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Traitement des donnees Medallion **Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3188fc7e-0d77-4e85-989c-5008ef20bef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Couche BRONZE (Raw)\n",
    "Chargement des donnees au format Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efac98a4-ca8c-4a01-b859-d0c9e2218ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_raw\n",
    "\n",
    "# Write Bronze (raw) as Delta\n",
    "bronze_path = f\"{BRONZE}/delta\"\n",
    "(df.write\n",
    "   .format(\"delta\")\n",
    "   .mode(\"overwrite\")\n",
    "   .save(bronze_path))\n",
    "\n",
    "# Read back\n",
    "bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "print(\"bronze rows:\", bronze.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc14b06-630d-46d2-bfc6-7c4ecc246aeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Couche Silver (Cleaned)\n",
    "nettoyage et standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f8288d-773d-4d6f-8428-a5b5d5e8ce65",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 17"
    }
   },
   "outputs": [],
   "source": [
    "### Silver Goals\n",
    "#- Remove invalid rows (negative distance, negative duration, negative totals)\n",
    "#- Standardize derived columns (trip_date, trip_hour, trip_duration_min)\n",
    "bronze = spark.read.format(\"delta\").load(bronze_path)\n",
    "\n",
    "silver = (bronze\n",
    "  .withColumn(\"trip_date\", F.to_date(\"tpep_pickup_datetime\"))\n",
    "  .withColumn(\"trip_hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "  .withColumn(\"trip_duration_min\",\n",
    "      (F.unix_timestamp(\"tpep_dropoff_datetime\") - F.unix_timestamp(\"tpep_pickup_datetime\")) / 60.0\n",
    "  )\n",
    "  .filter(\"trip_distance > 0\")\n",
    "  .filter(\"trip_duration_min > 0\")\n",
    "  .filter(\"total_amount > 0\")\n",
    "  .dropna(subset=[\"trip_date\",\"trip_distance\",\"total_amount\"])\n",
    ")\n",
    "\n",
    "silver_path = f\"{SILVER}/delta\"\n",
    "silver.write.format(\"delta\").mode(\"overwrite\").save(silver_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbc1e98e-254c-417c-8da7-e40e7be81092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Couche Gold (Curated)\n",
    "Donnees agrégées pour l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42afbc34-26f7-47d4-b038-86010352cbbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create analytics-ready KPI tables (daily + by pickup/dropoff).\n",
    "silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "gold_daily = (silver.groupBy(\"trip_date\")\n",
    "  .agg(F.count(\"*\").alias(\"trips\"),\n",
    "       F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "       F.avg(\"total_amount\").alias(\"avg_total\"),\n",
    "       F.sum(\"total_amount\").alias(\"sum_total\"))\n",
    ")\n",
    "\n",
    "gold_daily_path = f\"{GOLD}/daily\"\n",
    "gold_daily.write.format(\"delta\").mode(\"overwrite\").save(gold_daily_path)\n",
    "\n",
    "display(spark.read.format(\"delta\").load(gold_daily_path).orderBy(\"trip_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd96dd36-0d9f-4459-9995-da1e443d2fb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### New files arrive\n",
    "How to add datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283fc161-ef7b-496b-a74b-88539ed4ee2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 19"
    }
   },
   "outputs": [],
   "source": [
    "#**Scenario:** New monthly files arrive. Some trips may overlap (reprocessing). We deduplicate using a synthetic key.\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.read.format(\"delta\").load(f\"{SILVER}/delta\")\n",
    "\n",
    "# Create a synthetic trip key (example; adjust to your schema)\n",
    "# In practice, build a stable unique key if the dataset provides one.\n",
    "silver_keyed = silver.withColumn(\n",
    "    \"trip_key\",\n",
    "    F.sha2(F.concat_ws(\"||\",\n",
    "        F.col(\"tpep_pickup_datetime\").cast(\"string\"),\n",
    "        F.col(\"tpep_dropoff_datetime\").cast(\"string\"),\n",
    "        F.col(\"PULocationID\").cast(\"string\"),\n",
    "        F.col(\"DOLocationID\").cast(\"string\"),\n",
    "        F.col(\"total_amount\").cast(\"string\")\n",
    "    ), 256)\n",
    ")\n",
    "\n",
    "target_path = f\"{SILVER}/dedup\"\n",
    "silver_keyed.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd88c1b6-1d6b-4168-9924-266ecaa9171e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Simulating new datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba221b27-df11-4ae3-b094-6474ad8461ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Simulating, Babacar to add more explanation during  Class\n",
    "'''\n",
    "updates = silver_keyed.limit(1000)  # replace with real incremental month read\n",
    "\n",
    "updates.createOrReplaceTempView(\"updates\")\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS silver_dedup USING DELTA LOCATION '{target_path}'\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO silver_dedup t\n",
    "USING updates s\n",
    "ON t.trip_key = s.trip_key\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d0c4944-356a-4f9e-878a-83f640a71582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Auto Loader Ingestion (File-Based Incremental Ingestion)\n",
    "\n",
    "Auto Loader provides a Structured Streaming source `cloudFiles` that incrementally processes new files as they arrive, optionally processing existing files, and is designed to scale to very large file volumes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0f92ab-d8fc-40ce-aadb-12c692e3bc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_path = f\"{BRONZE}/incoming_parquet\"         # you drop new files here\n",
    "schema_path = f\"{BRONZE}/_schemas/yellow\"         # schema location\n",
    "checkpoint = f\"{CHECKPOINTS}/autoloader_yellow\"   # checkpoint\n",
    "\n",
    "dbutils.fs.mkdirs(input_path)\n",
    "dbutils.fs.mkdirs(schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbc83a0f-4a9e-478c-81f4-7e9ff3c05998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_stream_path = f\"{BRONZE}/autoloader_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63477cc8-8d41-49e9-b94a-e8fa553ac7df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# add a file in the bronze autoloader_date first\n",
    "file_name = \"yellow_tripdata_2024-03.parquet\"\n",
    "\n",
    "# file_url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file_name}\"\n",
    "file_url = f\"/Volumes/workspace/ipsldic3/mainvol/yellostaxidataset20024/{file_name}\"\n",
    "local_path = f\"{input_path}/{file_name}\"\n",
    "\n",
    "#file_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-02.parquet\"\n",
    "#local_path = f\"{input_path}/yellow_2024_02.parquet\"\n",
    "\n",
    "# Use Python to run shell commands for dynamic paths\n",
    "import subprocess\n",
    "\n",
    "# subprocess.run(f\"wget -q -O {local_path} {file_url}\", shell=True, check=True)\n",
    "subprocess.run(f\"cp {file_url} {local_path}\", shell=True, check=True)\n",
    "subprocess.run(f\"ls -lh {input_path}\", shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18da581b-3384-497a-9674-f1b085eef638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Streaming read with Auto Loader (Parquet), needs another cluster type\n",
    "'''stream_df = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "  .load(input_path)\n",
    ")\n",
    "\n",
    "# Write stream to Bronze Delta\n",
    "\n",
    "q = (stream_df.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"append\")\n",
    "  .option(\"checkpointLocation\", checkpoint)\n",
    "  .start(bronze_stream_path)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1242aa-41b2-489c-999e-bcd315ca3d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Performance Engineering (Partitioning, Caching, File Layout)\n",
    "\n",
    "**Tasks:**\n",
    "- Partition Gold tables by date\n",
    "- Compare query time before/after caching\n",
    "- Inspect physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49ef63bb-a50f-4d90-b2bd-022eff0063d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 27"
    }
   },
   "outputs": [],
   "source": [
    "silver = spark.read.format(\"delta\").load(f\"{SILVER}/delta\")\n",
    "\n",
    "# Partitioned write (example)\n",
    "gold_by_date_path = f\"{GOLD}/by_date\"\n",
    "(silver.write\n",
    "  .format(\"delta\")\n",
    "  .mode(\"overwrite\")\n",
    "  .partitionBy(\"trip_date\")\n",
    "  .save(gold_by_date_path))\n",
    "\n",
    "# Benchmark sample query\n",
    "import time\n",
    "t0 = time.time()\n",
    "spark.read.format(\"delta\").load(gold_by_date_path).filter(\"trip_date = '2024-01-15'\").count()\n",
    "print(\"secs:\", time.time() - t0)\n",
    "\n",
    "# Cache benchmark (not supported on serverless)\n",
    "# dfp = spark.read.format(\"delta\").load(gold_by_date_path).cache()\n",
    "# dfp.count()\n",
    "# t1 = time.time()\n",
    "# dfp.filter(\"trip_date = '2024-01-15'\").count()\n",
    "# print(\"cached secs:\", time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c4cc46-b1b8-48be-a367-0bbdb37927a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Module 10 — Structured Streaming (Near-Real-Time KPIs)\n",
    "\n",
    "We compute streaming aggregates (trips/hour) and write to a Delta sink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67dc3164-e011-4cdf-92be-c294ca710b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' Bench not supported\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "stream_src = f\"{BRONZE}/incoming_parquet\"\n",
    "schema_path = f\"{BRONZE}/_schemas/yellow_stream_kpi\"\n",
    "checkpoint = f\"{CHECKPOINTS}/kpi_hourly\"\n",
    "\n",
    "# Read arriving Parquet files as a stream\n",
    "sdf = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  .option(\"cloudFiles.format\", \"parquet\")\n",
    "  .option(\"cloudFiles.schemaLocation\", schema_path)\n",
    "  .load(stream_src)\n",
    ")\n",
    "\n",
    "# Feature columns\n",
    "sdf2 = (sdf\n",
    "  .withColumn(\"pickup_ts\", F.col(\"tpep_pickup_datetime\"))\n",
    "  .withColumn(\"trip_hour_ts\", F.date_trunc(\"hour\", F.col(\"pickup_ts\")))\n",
    ")\n",
    "\n",
    "hourly_kpi = (sdf2.groupBy(\"trip_hour_ts\")\n",
    "  .agg(F.count(\"*\").alias(\"trips\"),\n",
    "       F.avg(\"total_amount\").alias(\"avg_total\"))\n",
    ")\n",
    "\n",
    "out_path = f\"{GOLD}/stream_hourly_kpi\"\n",
    "\n",
    "q = (hourly_kpi.writeStream\n",
    "  .format(\"delta\")\n",
    "  .outputMode(\"complete\")  # aggregates often use complete mode\n",
    "  .option(\"checkpointLocation\", checkpoint)\n",
    "  .start(out_path)\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a943d88e-f0bb-4e01-b798-cbb654cd1e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLlib 1: Feature Engineering + ML Pipelines\n",
    "Goal: Create a reusable feature pipeline that outputs a `features` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4d1228-5b1a-4a32-8f17-f7cb34fb3d51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "silver = spark.read.format(\"delta\").load(f\"{SILVER}/delta\")\n",
    "\n",
    "base = (silver\n",
    "  .select(\"trip_distance\",\"passenger_count\",\"PULocationID\",\"DOLocationID\",\n",
    "          \"payment_type\",\"trip_duration_min\",\"total_amount\",\"trip_date\")\n",
    "  .dropna()\n",
    "  .filter(\"trip_distance > 0 AND trip_duration_min > 0 AND total_amount > 0\")\n",
    ")\n",
    "\n",
    "cat_cols = [\"payment_type\", \"PULocationID\", \"DOLocationID\"]\n",
    "num_cols = [\"trip_distance\", \"passenger_count\", \"trip_duration_min\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\") for c in cat_cols]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{c}_ohe\" for c in cat_cols] + num_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipe = Pipeline(stages=indexers + encoders + [assembler])\n",
    "feat_model = pipe.fit(base)\n",
    "featured = feat_model.transform(base)\n",
    "\n",
    "display(featured.select(\"features\",\"total_amount\",\"trip_date\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "506c7806-ab6b-4631-86d2-e240bd1412a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## MLlib 2: Modeling (Regression + Classification) + Evaluation\n",
    "\n",
    "**Two tasks:**\n",
    "1. Regression: predict `total_amount`\n",
    "2. Classification: predict whether `tip_amount > 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d4262c-9c2a-41c0-978d-827fa1be5cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "silver = spark.read.format(\"delta\").load(f\"{SILVER}/delta\")\n",
    "\n",
    "# Build base with label columns\n",
    "df_ml = (silver\n",
    "  .select(\"trip_distance\",\"passenger_count\",\"PULocationID\",\"DOLocationID\",\n",
    "          \"payment_type\",\"trip_duration_min\",\"total_amount\",\"tip_amount\",\"trip_date\")\n",
    "  .dropna()\n",
    "  .filter(\"trip_distance > 0 AND trip_duration_min > 0 AND total_amount > 0\")\n",
    "  .withColumn(\"label_tip\", (F.col(\"tip_amount\") > 0).cast(\"double\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da9b0f47-a625-4f3c-a61e-0dcf73a880f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reuse pipeline logic from Module MLlib1\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "cat_cols = [\"payment_type\", \"PULocationID\", \"DOLocationID\"]\n",
    "num_cols = [\"trip_distance\", \"passenger_count\", \"trip_duration_min\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_idx\", outputCol=f\"{c}_ohe\") for c in cat_cols]\n",
    "assembler = VectorAssembler(inputCols=[f\"{c}_ohe\" for c in cat_cols] + num_cols, outputCol=\"features\")\n",
    "\n",
    "pipe = Pipeline(stages=indexers + encoders + [assembler])\n",
    "pipe_model = pipe.fit(df_ml)\n",
    "feat = pipe_model.transform(df_ml)\n",
    "\n",
    "# Time-aware split\n",
    "dates = [r[0] for r in feat.select(\"trip_date\").distinct().orderBy(\"trip_date\").collect()]\n",
    "cut = dates[int(len(dates)*0.8)]\n",
    "\n",
    "train = feat.filter(F.col(\"trip_date\") < F.lit(cut))\n",
    "test  = feat.filter(F.col(\"trip_date\") >= F.lit(cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a61a22f-0b50-46b5-a27b-95f07184ea90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Regression: Total Amount\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"total_amount\", maxIter=5, maxDepth=6)\n",
    "gbt_model = gbt.fit(train)\n",
    "pred = gbt_model.transform(test)\n",
    "\n",
    "rmse = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\").evaluate(pred)\n",
    "mae  = RegressionEvaluator(labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"mae\").evaluate(pred)\n",
    "print(\"Regression RMSE:\", rmse, \"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a7f4b4-8112-4db3-90be-f6955915324a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### Classification: Tip / No-Tip\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label_tip\", maxIter=30, regParam=0.01)\n",
    "lr_model = lr.fit(train)\n",
    "pred2 = lr_model.transform(test)\n",
    "\n",
    "auc = BinaryClassificationEvaluator(labelCol=\"label_tip\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\").evaluate(pred2)\n",
    "print(\"Classification AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbe63fc1-d254-41f1-ae07-16170bad47b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "capstone-project-ipsl-dic3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
